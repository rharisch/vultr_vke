name: Manage Vultr Kubernetes Engine

on:
  workflow_dispatch:
    inputs:
      clustername:
        description: 'Cluster Name'
        required: true
        type: string
      action:
        description: 'Action to perform'
        required: true
        default: 'deploy'
        type: choice
        options:
          - deploy
          - destroy

jobs:
  deploy:
    if: github.event_name != 'workflow_dispatch' || github.event.inputs.action == 'deploy'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.5.2

    - name: Setup Kubernetes Tools
      uses: yokawasa/action-setup-kube-tools@v0.11.1
      with:
        setup-tools: |
          kubectl
          helm

    - name: Authenticate to Vultr
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
      run: |
        echo $VULTR_API_KEY > ~/.vultr_token

    - name: Initialize Terraform
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: terraform init
      working-directory: ./terraform/cluster

    - name: Plan Terraform deployment
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        TF_VAR_cluster_name: ${{ github.event.inputs.clustername }}
      run: terraform plan -out=tfplan
      working-directory: ./terraform/cluster

    - name: Apply Terraform deployment
      if: github.ref == 'refs/heads/main'
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: terraform apply -auto-approve tfplan
      working-directory: ./terraform/cluster

    - name: Extract Output Variables from Terraform
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        mkdir -p ~/.kube
        terraform-bin -chdir="./terraform/cluster" output --raw vultr_kube_config | base64 -d > ~/.kube/config
        chmod 0600 ~/.kube/config
        echo "VKE_CLUSTER_ID=$(terraform-bin -chdir="./terraform/cluster" output --raw vultr_vke_cluster_id)" >> $GITHUB_ENV
        echo "VKE_ENDPOINT=$(terraform-bin -chdir="./terraform/cluster" output --raw vultr_vke_endpoint)" >> $GITHUB_ENV

    - name: Wait for Control plane node to finish booting
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        VKE_CLUSTER_ID: ${{ env.VKE_CLUSTER_ID }}
      run: |
        curl --no-progress-meter -s -H "Authorization: Bearer ${VULTR_API_KEY}" "https://api.vultr.com/v2/kubernetes/clusters/${VKE_CLUSTER_ID}"
        CONTROL_PLANE_NODE_ID=$(curl --no-progress-meter -s -H "Authorization: Bearer ${VULTR_API_KEY}" "https://api.vultr.com/v2/kubernetes/clusters/${VKE_CLUSTER_ID}" | jq -r '.vke_cluster.node_pools[].nodes[].id')
        loopcounter=0
        until [[ "$(curl --no-progress-meter -s -H "Authorization: Bearer ${VULTR_API_KEY}" "http://api.vultr.com/v2/instances/${CONTROL_PLANE_NODE_ID}" | jq -r '.instance.server_status')" == "ok" || $loopcounter -gt 20 ]]; do
          ((loopcounter++))
          echo "Waiting for controlplane node to finish booting"
          sleep 60
        done
        if [[ $loopcounter -gt 10 ]]; then exit 1; fi

    - name: Wait for API to become available
      env:
        VKE_ENDPOINT: "https://${{ env.VKE_ENDPOINT }}:6443/healthz"
      shell: bash
      run: |
        curl -k --retry 10 --retry-delay 60 --retry-all-errors "${VKE_ENDPOINT}"

    - name: Install Cert-Manager
      shell: bash
      run: |
        kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.15.1/cert-manager.yaml
        kubectl create secret generic vultr-credentials --from-literal=apiKey=${{ secrets.VULTR_API_KEY }} --namespace cert-manager || echo "Secret already exists"
        git clone --depth=1 https://github.com/vultr/cert-manager-webhook-vultr.git ./vultr-webhook

        # Avoid a race condition where vultr webhook tries to connect before cert-manager-webhook is ready and fails install process
        loopcounter=0
        until [[ $(kubectl get deployments/cert-manager-webhook -n cert-manager -o jsonpath='{.status.readyReplicas}') -gt 0 || $loopcounter -gt 10 ]]; do
          ((loopcounter++))
          echo "Waiting for webhook dependency"
          sleep 5
        done
        if [[ $loopcounter -gt 10 ]]; then exit 1; fi

        ls -l ./vultr-webhook/deploy
        helm upgrade --install --namespace cert-manager cert-manager-webhook-vultr ./vultr-webhook/deploy/cert-manager-webhook-vultr
        cat <<EOF | oc apply -f -
        apiVersion: cert-manager.io/v1
        kind: ClusterIssuer
        metadata:
          annotations:
            meta.helm.sh/release-name: cert-manager-webhook-vultr
            meta.helm.sh/release-namespace: cert-manager
          labels:
            app.kubernetes.io/managed-by: Helm
          name: vultr-letsencrypt-prod
        spec:
          acme:
            email: ${{ secrets.admin_email }}
            privateKeySecretRef:
              name: vultr-letsencrypt-prod
            server: https://acme-v02.api.letsencrypt.org/directory
            solvers:
            - dns01:
                webhook:
                  config:
                    apiKeySecretRef:
                      key: apiKey
                      name: vultr-credentials
                  groupName: acme.vultr.com
                  solverName: vultr
        EOF
        cat <<EOF | oc apply -f -
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          name: cert-manager-webhook-vultr:secret-reader
          namespace: cert-manager
        rules:
        - apiGroups: [""]
          resources: ["secrets"]
          resourceNames: ["vultr-credentials"]
          verbs: ["get", "watch"]
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          name: cert-manager-webhook-vultr:secret-reader
          namespace: cert-manager
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: cert-manager-webhook-vultr:secret-reader
        subjects:
          - apiGroup: ""
            kind: ServiceAccount
            name: cert-manager-webhook-vultr
        EOF
    
    - name: Install ExternalDNS
      shell: bash
      run: |
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: external-dns
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          name: external-dns
        rules:
        - apiGroups: [""]
          resources: ["services","endpoints","pods"]
          verbs: ["get","watch","list"]
        - apiGroups: ["extensions","networking.k8s.io"]
          resources: ["ingresses"]
          verbs: ["get","watch","list"]
        - apiGroups: [""]
          resources: ["nodes"]
          verbs: ["list"]
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: external-dns-viewer
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: external-dns
        subjects:
        - kind: ServiceAccount
          name: external-dns
          namespace: default
        ---
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: external-dns
        spec:
          strategy:
            type: Recreate
          selector:
            matchLabels:
              app: external-dns
          template:
            metadata:
              labels:
                app: external-dns
            spec:
              serviceAccountName: external-dns
              containers:
              - name: external-dns
                image: k8s.gcr.io/external-dns/external-dns:v0.10.2
                args:
                - --source=ingress  #service is also possible
                - --domain-filter=${{ vars.EXTERNAL_DNS_DOMAIN_NAME }}
                - --provider=vultr
                - --registry=txt
                - --txt-owner-id=${{ github.event.inputs.clustername }}-automation
                env:
                - name: VULTR_API_KEY
                  value: "${{ secrets.VULTR_API_KEY }}" # Enter your Vultr API Key
        EOF

    - name: Install Nginx Ingress Controller
      shell: bash
      run: |
        helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
        helm repo update
        helm upgrade --install ingress-nginx --set rbac.create=true --set "controller.extraArgs.enable-ssl-passthrough=" ingress-nginx/ingress-nginx

    - name: Deploy ArgoCD Chart
      shell: bash
      run: |
        cat <<EOF >./argocd-values.yaml
        global:
          domain: argocd.${{ github.event.inputs.clustername }}.${{ vars.EXTERNAL_DNS_DOMAIN_NAME }}
        server:
          certificate:
            enabled: true
            secretName: argocd-server-tls
            domain: argocd.${{ github.event.inputs.clustername }}.${{ vars.EXTERNAL_DNS_DOMAIN_NAME }}
            issuer:
              group:  "cert-manager.io"
              kind: "ClusterIssuer"
              name: "vultr-letsencrypt-staging"
          ingress:
            enabled: true
            annotations:
              nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
              nginx.ingress.kubernetes.io/ssl-passthrough: "true"
              external-dns.alpha.kubernetes.io/hostname: argocd.${{ github.event.inputs.clustername }}.${{ vars.EXTERNAL_DNS_DOMAIN_NAME }}

            ingressClassName: nginx
        EOF
        helm repo add argocd https://argoproj.github.io/argo-helm
        helm repo up
        helm upgrade --install --namespace argocd --create-namespace argocd argocd/argo-cd -f ./argocd-values.yaml

  destroy:
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'destroy'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.5.2

#    - name: Install vultr-cli
#      uses: techknowlogick/action-vultr@v2
#      with:
#        token: ${{ secrets.VULTR_API_KEY }}

    - name: Authenticate to Vultr
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
      run: |
        echo $VULTR_API_KEY > ~/.vultr_token

#    - name: Get Object IDs for Kubernetes resources to destroy after Terraform
#      env:
#        CLUSTERNAME: "${{ github.event.inputs.clustername }}"
#      shell: bash
#      run: |
#        KUBE_INSTANCE_ID=$(vultr-cli kubernetes list -o json | jq -r --arg CLUSTERNAME "${CLUSTERNAME}" '.vke_clusters[] | select(.label=$CLUSTERNAME) | .id')
#        VPC_ID=$(vultr-cli vpc list -o json | jq  --arg KUBE_INSTANCE_ID "${KUBE_INSTANCE_ID}" -r '.vpcs[] | select(.description | test($KUBE_INSTANCE_ID)) | .id')
#        echo "VPD_ID=${VPC_ID}" >> $GITHUB_ENV

    - name: Initialize Terraform
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: terraform init
      working-directory: ./terraform/cluster

    - name: Plan Terraform destruction
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        TF_VAR_cluster_name: ${{ github.event.inputs.clustername }}
      run: terraform plan -destroy -out=tfdestroy
      working-directory: ./terraform/cluster

    - name: Apply Terraform destruction
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: terraform apply -auto-approve tfdestroy
      working-directory: ./terraform/cluster

